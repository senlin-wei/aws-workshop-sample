{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文本分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分词\n",
    "# 词语 -> id\n",
    "#   matrix -> [|V|, embed_size]\n",
    "#   词语A -> id(5)\n",
    "#   词表\n",
    "\n",
    "# label -> id\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import jieba # pip install jieba\n",
    "import urllib \n",
    "base_dir = './data/cnews/'\n",
    "\n",
    "# input files\n",
    "train_file = base_dir + 'cnews.train.txt'\n",
    "val_file = base_dir + 'cnews.val.txt'\n",
    "test_file = base_dir + 'cnews.test.txt'\n",
    "\n",
    "# output files\n",
    "seg_train_file = base_dir + 'cnews.train.seg.txt'\n",
    "seg_val_file = base_dir + 'cnews.val.seg.txt'\n",
    "seg_test_file = base_dir + 'cnews.test.seg.txt'\n",
    "\n",
    "vocab_file = base_dir + 'cnews.vocab.txt'\n",
    "category_file = base_dir + 'cnews.category.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "!pwd\n",
    "if not os.path.exists(train_file):\n",
    "    print('{}不存在, 开始下载文件'.format(train_file))\n",
    "    urllib.request.urlretrieve('https://s3.amazonaws.com/dikers.nwcd/data-set/cnews_data.zip', \"cnews_data.zip\")\n",
    "    !unzip ./cnews_data.zip\n",
    "    !rm ./cnews_data.zip\n",
    "    !mkdir ./data/cnews \n",
    "    !mv cnews.train.txt ./data/cnews/\n",
    "    !mv cnews.test.txt ./data/cnews/\n",
    "    !mv cnews.val.txt ./data/cnews/\n",
    "else:\n",
    "    print('文件已经存在')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用jieba分词\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(val_file, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "label, content = lines[0].strip('\\r\\n').split('\\t')\n",
    "word_iter = jieba.cut(content)\n",
    "\n",
    "print('label', label)\n",
    "print(content)\n",
    "print('/ '.join(word_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  将样本文件分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_seg_file(input_file, output_seg_file):\n",
    "    \"\"\"Segment the sentences in each line in input_file\"\"\"\n",
    "    with open(input_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    with open(output_seg_file, 'w') as f:\n",
    "        for line in lines:\n",
    "            label, content = line.strip('\\r\\n').split('\\t')\n",
    "            word_iter = jieba.cut(content)\n",
    "            word_content = ''\n",
    "            for word in word_iter:\n",
    "                word = word.strip(' ')\n",
    "                if word != '':\n",
    "                    word_content += word + ' '\n",
    "            out_line = '%s\\t%s\\n' % (label, word_content.strip(' '))\n",
    "            f.write(out_line)\n",
    "        print('{} 文件分割完成, 输出路径{} .'.format(input_file, output_seg_file))\n",
    "        \n",
    "if not os.path.exists(seg_train_file):\n",
    "    generate_seg_file(train_file, seg_train_file)\n",
    "if not os.path.exists(seg_val_file):\n",
    "    generate_seg_file(val_file, seg_val_file)\n",
    "if not os.path.exists(seg_test_file):\n",
    "    generate_seg_file(test_file, seg_test_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成词汇表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def generate_vocab_file(input_seg_file, output_vocab_file):\n",
    "    with open(input_seg_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    word_dict = {}\n",
    "    for line in lines:\n",
    "        label, content = line.strip('\\r\\n').split('\\t')\n",
    "        for word in content.split():\n",
    "            word_dict.setdefault(word, 0)\n",
    "            word_dict[word] += 1\n",
    "    # [(word, frequency), ..., ()]\n",
    "    sorted_word_dict = sorted(\n",
    "        word_dict.items(), key = lambda d:d[1], reverse=True)\n",
    "    with open(output_vocab_file, 'w') as f:\n",
    "        f.write('<UNK>\\t10000000\\n')\n",
    "        for item in sorted_word_dict:\n",
    "            f.write('%s\\t%d\\n' % (item[0], item[1]))\n",
    "\n",
    "generate_vocab_file(seg_train_file, vocab_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vocab file 格式： \n",
    "词语 和 出现的数量\n",
    "\n",
    "```\n",
    "生活\t13141\n",
    "能够\t12911\n",
    "不会\t12898\n",
    "不同\t12871\n",
    "获得\t12870\n",
    "城市\t12825\n",
    "学校\t12775\n",
    "一定\t12736\n",
    "一直\t12606\n",
    "上海\t12574\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 对应的label 标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def generate_category_dict(input_file, category_file):\n",
    "    with open(input_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    category_dict = {}\n",
    "    for line in lines:\n",
    "        label, content = line.strip('\\r\\n').split('\\t')\n",
    "        category_dict.setdefault(label, 0)\n",
    "        category_dict[label] += 1\n",
    "    category_number = len(category_dict)\n",
    "    with open(category_file, 'w') as f:\n",
    "        for category in category_dict:\n",
    "            line = '%s\\n' % category\n",
    "            print('%s\\t%d' % (category, category_dict[category]))\n",
    "            f.write(line)\n",
    "            \n",
    "generate_category_dict(train_file, category_file)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "num_word_threshold = 10\n",
    "num_timesteps = 50\n",
    "class Vocab:\n",
    "    def __init__(self, filename, num_word_threshold):\n",
    "        self._word_to_id = {}\n",
    "        self._unk = -1\n",
    "        self._num_word_threshold = num_word_threshold\n",
    "        self._read_dict(filename)\n",
    "    \n",
    "    def _read_dict(self, filename):\n",
    "        with open(filename, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            word, frequency = line.strip('\\r\\n').split('\\t')\n",
    "            frequency = int(frequency)\n",
    "            if frequency < self._num_word_threshold:\n",
    "                continue\n",
    "            idx = len(self._word_to_id)\n",
    "            if word == '<UNK>':\n",
    "                self._unk = idx\n",
    "            self._word_to_id[word] = idx\n",
    "    \n",
    "    def word_to_id(self, word):\n",
    "        return self._word_to_id.get(word, self._unk)\n",
    "    \n",
    "    def get_word_dict(self):\n",
    "        return self._word_to_id\n",
    "    \n",
    "    @property\n",
    "    def unk(self):\n",
    "        return self._unk\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self._word_to_id)\n",
    "    \n",
    "    def sentence_to_id(self, sentence):\n",
    "        word_ids = [self.word_to_id(cur_word) \\\n",
    "                    for cur_word in sentence.split()]\n",
    "        return word_ids\n",
    "\n",
    "\n",
    "class CategoryDict:\n",
    "    def __init__(self, filename):\n",
    "        self._category_to_id = {}\n",
    "        with open(filename, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            category = line.strip('\\r\\n')\n",
    "            idx = len(self._category_to_id)\n",
    "            self._category_to_id[category] = idx\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self._category_to_id)\n",
    "        \n",
    "    def category_to_id(self, category):\n",
    "        if not category in self._category_to_id:\n",
    "            raise Execption(\n",
    "                \"%s is not in our category list\" % category_name)\n",
    "        return self._category_to_id[category]\n",
    "        \n",
    "vocab = Vocab(vocab_file, num_word_threshold)\n",
    "vocab_size = vocab.size()\n",
    "print('vocab_size: %d' % vocab_size)\n",
    "\n",
    "category_vocab = CategoryDict(category_file)\n",
    "num_classes = category_vocab.size()\n",
    "print('num_classes: %d' % num_classes)\n",
    "test_str = '时尚'\n",
    "print(\n",
    "    'label: %s, id: %d' % (\n",
    "        test_str,\n",
    "        category_vocab.category_to_id(test_str)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成词的概率分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import random\n",
    "import numpy as np\n",
    "word_counts = vocab.get_word_dict()\n",
    "threshold = 1e-5\n",
    "print(word_counts['的'])\n",
    "total_count = len(word_counts)\n",
    "print('total_count', total_count)\n",
    "freqs = {word: count/total_count for word, count in word_counts.items()}\n",
    "print(freqs['儿童'])\n",
    "# p_drop = {word: 1 - np.sqrt(threshold/(freqs[word] + 1e-10)) for word in word_counts}\n",
    "p_drop = {vocab.word_to_id(word): 1 - np.sqrt(threshold/(freqs[word] + 1e-10)) for word in word_counts}\n",
    "print(p_drop[100]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import random\n",
    "class TextDataSet:\n",
    "    def __init__(self, filename, vocab, category_vocab, num_timesteps):\n",
    "        self._vocab = vocab\n",
    "        self._category_vocab = category_vocab\n",
    "        self._num_timesteps = num_timesteps\n",
    "        # matrix\n",
    "        self._inputs = []\n",
    "        # vector\n",
    "        self._outputs = []\n",
    "        self._indicator = 0\n",
    "        self._parse_file(filename)\n",
    "    \n",
    "    def _parse_file(self, filename):\n",
    "        print('Loading data from %s', filename)\n",
    "        with open(filename, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            label, content = line.strip('\\r\\n').split('\\t')\n",
    "            id_label = self._category_vocab.category_to_id(label)\n",
    "\n",
    "            id_words = self._vocab.sentence_to_id(content)\n",
    "            # 过滤掉一部分概率比较低的值\n",
    "#             print(len(content) , len(id_words))\n",
    "            id_words = [word for word in id_words if random.random() > (1 - p_drop[word])]\n",
    "\n",
    "            id_words = id_words[0: self._num_timesteps]\n",
    "            padding_num = self._num_timesteps - len(id_words)\n",
    "            id_words = id_words + [\n",
    "                self._vocab.unk for i in range(padding_num)]\n",
    "            self._inputs.append(id_words)\n",
    "            self._outputs.append(id_label)\n",
    "        self._inputs = np.asarray(self._inputs, dtype = np.int32)\n",
    "        self._outputs = np.asarray(self._outputs, dtype = np.int32)\n",
    "        self._random_shuffle()\n",
    "        self._num_examples = len(self._inputs)\n",
    "    \n",
    "    def _random_shuffle(self):\n",
    "        p = np.random.permutation(len(self._inputs))\n",
    "        self._inputs = self._inputs[p]\n",
    "        self._outputs = self._outputs[p]\n",
    "    \n",
    "    def num_examples(self):\n",
    "        return self._num_examples\n",
    "    \n",
    "    def next_batch(self, batch_size):\n",
    "        end_indicator = self._indicator + batch_size\n",
    "        if end_indicator > len(self._inputs):\n",
    "            self._random_shuffle()\n",
    "            self._indicator = 0\n",
    "            end_indicator = batch_size\n",
    "        if end_indicator > len(self._inputs):\n",
    "            raise Execption(\"batch_size: %d is too large\" % batch_size)\n",
    "        \n",
    "        batch_inputs = self._inputs[self._indicator: end_indicator]\n",
    "        batch_outputs = self._outputs[self._indicator: end_indicator]\n",
    "        self._indicator = end_indicator\n",
    "        return batch_inputs, batch_outputs\n",
    "    \n",
    "    def get_data(self):\n",
    "        batch_inputs = self._inputs\n",
    "        batch_outputs = self._outputs\n",
    "        return batch_inputs, batch_outputs\n",
    "        \n",
    "            \n",
    "train_dataset = TextDataSet(\n",
    "    seg_train_file, vocab, category_vocab,num_timesteps) \n",
    "val_dataset = TextDataSet(\n",
    "    seg_val_file, vocab, category_vocab, num_timesteps)\n",
    "test_dataset = TextDataSet(\n",
    "    seg_test_file, vocab, category_vocab, num_timesteps)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from keras.utils import to_categorical\n",
    "print(train_dataset.num_examples())\n",
    "print(val_dataset.num_examples())\n",
    "print(test_dataset.num_examples())\n",
    "\n",
    "x_train , y_train = train_dataset.get_data()\n",
    "x_val, y_val = val_dataset.get_data()\n",
    "x_test, y_test  = test_dataset.get_data()\n",
    "y_train = to_categorical(y_train)\n",
    "y_val = to_categorical(y_val)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_val.shape)\n",
    "print(x_test.shape)\n",
    "print('y_train.shape', y_train.shape)\n",
    "print(x_train[5:8])\n",
    "print(type(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense, Dropout\n",
    "maxlen = x_test.shape[1]\n",
    "_epochs = 10\n",
    "model = Sequential()\n",
    "# We specify the maximum input length to our Embedding layer\n",
    "# so we can later flatten the embedded inputs\n",
    "model.add(Embedding(vocab_size, 32, input_length=maxlen))\n",
    "# After the Embedding layer, \n",
    "# our activations have shape `(samples, maxlen, 8)`.\n",
    "\n",
    "# We flatten the 3D tensor of embeddings \n",
    "# into a 2D tensor of shape `(samples, maxlen * 8)`\n",
    "model.add(Flatten())\n",
    "# We add the classifier on top\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=_epochs,\n",
    "                    batch_size=64,\n",
    "                    validation_data=(x_val, y_val))\n",
    "\n",
    "print(model.evaluate(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "_epochs = 10\n",
    "epochs = range(1, _epochs+1)\n",
    "val_loss = history.history['val_loss']\n",
    "plt.plot(epochs, val_loss, 'b+', color='r',label='Model')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Training loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 8, input_length=maxlen))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=_epochs,\n",
    "                    batch_size=64,\n",
    "                    validation_data=(x_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
